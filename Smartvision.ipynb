{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasudevanpooja/Video-Relevance-Scorer/blob/main/Smartvision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98676d9f",
      "metadata": {
        "id": "98676d9f"
      },
      "outputs": [],
      "source": [
        "# pip install datasets pillow pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73c91a8",
      "metadata": {
        "id": "c73c91a8"
      },
      "outputs": [],
      "source": [
        "## STEP 1: IMPORTS AND CONFIGURATION\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25 Selected Classes with COCO Category IDs\n",
        "\n",
        "**Vehicles:** car(3), truck(8), bus(6), motorcycle(4), bicycle(2), airplane(5)  \n",
        "**Person:** person(1)  \n",
        "**Outdoor:** traffic light(10), stop sign(13), bench(15)  \n",
        "**Animals:** dog(18), cat(17), horse(19), bird(16), cow(21), elephant(22)  \n",
        "**Kitchen & Food:** bottle(44), cup(47), bowl(51), pizza(59), cake(61)  \n",
        "**Furniture:** chair(62), couch(63), bed(65), potted plant(64)"
      ],
      "metadata": {
        "id": "BYxZpR9EMSjN"
      },
      "id": "BYxZpR9EMSjN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531afa7c",
      "metadata": {
        "id": "531afa7c"
      },
      "outputs": [],
      "source": [
        "# 25 Selected Classes (COCO category IDs)\n",
        "\n",
        "SELECTED_CLASSES = {\n",
        "    'person': 1,\n",
        "    'bicycle': 2,\n",
        "    'car': 3,\n",
        "    'motorcycle': 4,\n",
        "    'airplane': 5,\n",
        "    'bus': 6,\n",
        "    'truck': 8,\n",
        "    'traffic light': 10,\n",
        "    'stop sign': 13,\n",
        "    'bench': 15,\n",
        "    'bird': 16,\n",
        "    'cat': 17,\n",
        "    'dog': 18,\n",
        "    'horse': 19,\n",
        "    'cow': 21,\n",
        "    'elephant': 22,\n",
        "    'bottle': 44,\n",
        "    'cup': 47,\n",
        "    'bowl': 51,\n",
        "    'pizza': 59,\n",
        "    'cake': 61,\n",
        "    'chair': 62,\n",
        "    'couch': 63,\n",
        "    'bed': 65,\n",
        "    'potted plant': 64\n",
        "}\n",
        "\n",
        "IMAGES_PER_CLASS = 100\n",
        "BASE_DIR = \"smartvision_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91700ca9",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e0b058bf2f6a4c6597809c8144c276bd"
          ]
        },
        "id": "91700ca9",
        "outputId": "5c118f1f-c82e-41e6-bba9-18c920b36ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Loading COCO dataset in STREAMING mode (no download)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0b058bf2f6a4c6597809c8144c276bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset loaded in streaming mode!\n"
          ]
        }
      ],
      "source": [
        "## STEP 2: LOAD COCO DATASET FROM HUGGING FACE\n",
        "\n",
        "print(\"ğŸ“¥ Loading COCO dataset in STREAMING mode (no download)...\")\n",
        "dataset = load_dataset(\"detection-datasets/coco\", split=\"train\", streaming=True)\n",
        "print(\"âœ… Dataset loaded in streaming mode!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3056a4a",
      "metadata": {
        "id": "c3056a4a",
        "outputId": "9e873ef2-5c58-4b59-d2b7-1b1d4d39d48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” Starting image collection from COCO dataset stream...\n",
            "ğŸ¯ Target: 100 images per class\n",
            "\n",
            "â³ Processing images from stream...\n",
            "ğŸ’¡ Progress updates every 100 images collected\n",
            "\n",
            "âœ“ Collected 100/2500 images\n",
            "âœ“ Collected 200/2500 images\n",
            "âœ“ Collected 300/2500 images\n",
            "âœ“ Collected 400/2500 images\n",
            "âœ“ Collected 500/2500 images\n",
            "âœ“ Collected 600/2500 images\n",
            "âœ“ Collected 700/2500 images\n",
            "âœ“ Collected 800/2500 images\n",
            "âœ“ Collected 900/2500 images\n",
            "âœ“ Collected 1000/2500 images\n",
            "âœ“ Collected 1100/2500 images\n",
            "âœ“ Collected 1200/2500 images\n",
            "ğŸ“Š Processed 1000 images | Collected 1290/2500\n",
            "âœ“ Collected 1300/2500 images\n",
            "âœ“ Collected 1400/2500 images\n",
            "âœ“ Collected 1500/2500 images\n",
            "âœ“ Collected 1600/2500 images\n",
            "âœ“ Collected 1700/2500 images\n",
            "âœ“ Collected 1800/2500 images\n",
            "âœ“ Collected 1900/2500 images\n",
            "âœ“ Collected 2000/2500 images\n",
            "ğŸ“Š Processed 2000 images | Collected 2087/2500\n",
            "âœ“ Collected 2100/2500 images\n",
            "âœ“ Collected 2200/2500 images\n",
            "âœ“ Collected 2300/2500 images\n",
            "ğŸ“Š Processed 3000 images | Collected 2328/2500\n",
            "ğŸ“Š Processed 4000 images | Collected 2382/2500\n",
            "âœ“ Collected 2400/2500 images\n",
            "ğŸ“Š Processed 5000 images | Collected 2432/2500\n",
            "ğŸ“Š Processed 6000 images | Collected 2463/2500\n",
            "ğŸ“Š Processed 7000 images | Collected 2478/2500\n",
            "ğŸ“Š Processed 8000 images | Collected 2481/2500\n",
            "ğŸ“Š Processed 9000 images | Collected 2490/2500\n",
            "ğŸ“Š Processed 10000 images | Collected 2498/2500\n",
            "âœ“ Collected 2500/2500 images\n",
            "ğŸ‰ Successfully collected 100 images for ALL classes!\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š COLLECTION COMPLETE:\n",
            "============================================================\n",
            "Images Processed: 10253\n",
            "Images Collected: 2500\n",
            "\n",
            "âœ… airplane            : 100 images\n",
            "âœ… bed                 : 100 images\n",
            "âœ… bench               : 100 images\n",
            "âœ… bicycle             : 100 images\n",
            "âœ… bird                : 100 images\n",
            "âœ… bottle              : 100 images\n",
            "âœ… bowl                : 100 images\n",
            "âœ… bus                 : 100 images\n",
            "âœ… cake                : 100 images\n",
            "âœ… car                 : 100 images\n",
            "âœ… cat                 : 100 images\n",
            "âœ… chair               : 100 images\n",
            "âœ… couch               : 100 images\n",
            "âœ… cow                 : 100 images\n",
            "âœ… cup                 : 100 images\n",
            "âœ… dog                 : 100 images\n",
            "âœ… elephant            : 100 images\n",
            "âœ… horse               : 100 images\n",
            "âœ… motorcycle          : 100 images\n",
            "âœ… person              : 100 images\n",
            "âœ… pizza               : 100 images\n",
            "âœ… potted plant        : 100 images\n",
            "âœ… stop sign           : 100 images\n",
            "âœ… traffic light       : 100 images\n",
            "âœ… truck               : 100 images\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## STEP 3: COLLECT IMAGES FROM STREAM\n",
        "\n",
        "print(\"\\nğŸ” Starting image collection from COCO dataset stream...\")\n",
        "print(f\"ğŸ¯ Target: {IMAGES_PER_CLASS} images per class\")\n",
        "print()\n",
        "\n",
        "# Initialize storage for collected images\n",
        "class_images = {class_name: [] for class_name in SELECTED_CLASSES.keys()}\n",
        "class_counts = {class_name: 0 for class_name in SELECTED_CLASSES.keys()}\n",
        "\n",
        "# Progress tracking\n",
        "total_collected = 0\n",
        "images_processed = 0\n",
        "max_iterations = 50000  # Safety limit\n",
        "\n",
        "print(\"â³ Processing images from stream...\")\n",
        "print(\"ğŸ’¡ Progress updates every 100 images collected\")\n",
        "print()\n",
        "\n",
        "# Iterate through streaming dataset\n",
        "for idx, item in enumerate(dataset):\n",
        "\n",
        "    images_processed += 1\n",
        "\n",
        "    # Progress update every 1000 images processed\n",
        "    if images_processed % 1000 == 0:\n",
        "        print(f\"ğŸ“Š Processed {images_processed} images | Collected {total_collected}/{len(SELECTED_CLASSES) * IMAGES_PER_CLASS}\")\n",
        "\n",
        "    # Safety check\n",
        "    if images_processed >= max_iterations:\n",
        "        print(f\"âš ï¸ Reached safety limit of {max_iterations} iterations\")\n",
        "        break\n",
        "\n",
        "    # Check if we have enough images for ALL classes\n",
        "    if all(count >= IMAGES_PER_CLASS for count in class_counts.values()):\n",
        "        print(\"ğŸ‰ Successfully collected 100 images for ALL classes!\")\n",
        "        break\n",
        "\n",
        "    # Get annotations from current image\n",
        "    annotations = item['objects']\n",
        "    categories = annotations['category']\n",
        "\n",
        "    # Check if any of our target classes are in this image\n",
        "    for cat_id in categories:\n",
        "        for class_name, class_id in SELECTED_CLASSES.items():\n",
        "            if cat_id == class_id and class_counts[class_name] < IMAGES_PER_CLASS:\n",
        "\n",
        "                # Store the ACTUAL image data (not just index!)\n",
        "                class_images[class_name].append({\n",
        "                    'image': item['image'],           # PIL Image object\n",
        "                    'annotations': item['objects'],   # Annotations\n",
        "                    'idx': images_processed           # For naming\n",
        "                })\n",
        "\n",
        "                class_counts[class_name] += 1\n",
        "                total_collected += 1\n",
        "\n",
        "                # Progress update every 100 collected\n",
        "                if total_collected % 100 == 0:\n",
        "                    print(f\"âœ“ Collected {total_collected}/{len(SELECTED_CLASSES) * IMAGES_PER_CLASS} images\")\n",
        "\n",
        "                break  # Only count once per class\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š COLLECTION COMPLETE:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Images Processed: {images_processed}\")\n",
        "print(f\"Images Collected: {total_collected}\")\n",
        "print()\n",
        "for class_name, count in sorted(class_counts.items()):\n",
        "    status = \"âœ…\" if count >= IMAGES_PER_CLASS else \"âš ï¸\"\n",
        "    print(f\"{status} {class_name:20s}: {count:3d} images\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216b344d",
      "metadata": {
        "id": "216b344d",
        "outputId": "3da8bbb1-7618-4d8d-bbf2-9e96a4d0dc06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ Creating project folder structure...\n",
            "\n",
            "âœ… Folder structure created successfully!\n",
            "\n",
            "ğŸ“‚ Structure:\n",
            "\n",
            "smartvision_dataset/\n",
            "â”œâ”€â”€ classification/\n",
            "â”‚   â”œâ”€â”€ train/\n",
            "â”‚   â”‚   â”œâ”€â”€ person/\n",
            "â”‚   â”‚   â”œâ”€â”€ car/\n",
            "â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n",
            "â”‚   â”œâ”€â”€ val/\n",
            "â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n",
            "â”‚   â””â”€â”€ test/\n",
            "â”‚       â””â”€â”€ ... (25 class folders)\n",
            "â”‚\n",
            "â””â”€â”€ detection/\n",
            "    â”œâ”€â”€ images/\n",
            "    â””â”€â”€ labels/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## STEP 4: CREATE FOLDER STRUCTURE\n",
        "\n",
        "print(\"\\nğŸ“ Creating project folder structure...\")\n",
        "print()\n",
        "\n",
        "# Create main directory\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for Classification task\n",
        "os.makedirs(f\"{BASE_DIR}/classification/train\", exist_ok=True)\n",
        "os.makedirs(f\"{BASE_DIR}/classification/val\", exist_ok=True)\n",
        "os.makedirs(f\"{BASE_DIR}/classification/test\", exist_ok=True)\n",
        "\n",
        "# Create subdirectories for Detection task\n",
        "os.makedirs(f\"{BASE_DIR}/detection/images\", exist_ok=True)\n",
        "os.makedirs(f\"{BASE_DIR}/detection/labels\", exist_ok=True)\n",
        "\n",
        "# Create class folders inside train/val/test\n",
        "for class_name in SELECTED_CLASSES.keys():\n",
        "    os.makedirs(f\"{BASE_DIR}/classification/train/{class_name}\", exist_ok=True)\n",
        "    os.makedirs(f\"{BASE_DIR}/classification/val/{class_name}\", exist_ok=True)\n",
        "    os.makedirs(f\"{BASE_DIR}/classification/test/{class_name}\", exist_ok=True)\n",
        "\n",
        "print(\"âœ… Folder structure created successfully!\")\n",
        "print()\n",
        "print(\"ğŸ“‚ Structure:\")\n",
        "print(f\"\"\"\n",
        "{BASE_DIR}/\n",
        "â”œâ”€â”€ classification/\n",
        "â”‚   â”œâ”€â”€ train/\n",
        "â”‚   â”‚   â”œâ”€â”€ person/\n",
        "â”‚   â”‚   â”œâ”€â”€ car/\n",
        "â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n",
        "â”‚   â”œâ”€â”€ val/\n",
        "â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n",
        "â”‚   â””â”€â”€ test/\n",
        "â”‚       â””â”€â”€ ... (25 class folders)\n",
        "â”‚\n",
        "â””â”€â”€ detection/\n",
        "    â”œâ”€â”€ images/\n",
        "    â””â”€â”€ labels/\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99b88ce",
      "metadata": {
        "id": "d99b88ce",
        "outputId": "81d02dbc-c46f-4457-e593-8d00db0e492d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ”€ Preparing Train/Val/Test splits...\n",
            "ğŸ“Š Split Ratio: 70% Train / 15% Val / 15% Test\n",
            "======================================================================\n",
            "\n",
            "person              : Train= 70 | Val=15 | Test=15\n",
            "bicycle             : Train= 70 | Val=15 | Test=15\n",
            "car                 : Train= 70 | Val=15 | Test=15\n",
            "motorcycle          : Train= 70 | Val=15 | Test=15\n",
            "airplane            : Train= 70 | Val=15 | Test=15\n",
            "bus                 : Train= 70 | Val=15 | Test=15\n",
            "truck               : Train= 70 | Val=15 | Test=15\n",
            "traffic light       : Train= 70 | Val=15 | Test=15\n",
            "stop sign           : Train= 70 | Val=15 | Test=15\n",
            "bench               : Train= 70 | Val=15 | Test=15\n",
            "bird                : Train= 70 | Val=15 | Test=15\n",
            "cat                 : Train= 70 | Val=15 | Test=15\n",
            "dog                 : Train= 70 | Val=15 | Test=15\n",
            "horse               : Train= 70 | Val=15 | Test=15\n",
            "cow                 : Train= 70 | Val=15 | Test=15\n",
            "elephant            : Train= 70 | Val=15 | Test=15\n",
            "bottle              : Train= 70 | Val=15 | Test=15\n",
            "cup                 : Train= 70 | Val=15 | Test=15\n",
            "bowl                : Train= 70 | Val=15 | Test=15\n",
            "pizza               : Train= 70 | Val=15 | Test=15\n",
            "cake                : Train= 70 | Val=15 | Test=15\n",
            "chair               : Train= 70 | Val=15 | Test=15\n",
            "couch               : Train= 70 | Val=15 | Test=15\n",
            "bed                 : Train= 70 | Val=15 | Test=15\n",
            "potted plant        : Train= 70 | Val=15 | Test=15\n"
          ]
        }
      ],
      "source": [
        "## STEP 5: TRAIN/VAL/TEST SPLIT (70/15/15)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ”€ Preparing Train/Val/Test splits...\")\n",
        "print(\"ğŸ“Š Split Ratio: 70% Train / 15% Val / 15% Test\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "# Initialize metadata dictionary\n",
        "metadata = {\n",
        "    'total_images': 0,\n",
        "    'classes': {},\n",
        "    'splits': {'train': 0, 'val': 0, 'test': 0}\n",
        "}\n",
        "\n",
        "# Create split dictionaries for each class\n",
        "train_data = {}\n",
        "val_data = {}\n",
        "test_data = {}\n",
        "\n",
        "# Process each class\n",
        "for class_name in SELECTED_CLASSES.keys():\n",
        "\n",
        "    all_items = class_images.get(class_name, [])\n",
        "\n",
        "    if not all_items:\n",
        "        print(f\"âš ï¸ Warning: No images found for {class_name}\")\n",
        "        continue\n",
        "\n",
        "    # Calculate split indices\n",
        "    n = len(all_items)\n",
        "    train_split = int(0.7 * n)   # 70% for training\n",
        "    val_split = int(0.85 * n)    # 15% for validation\n",
        "    # Remaining 15% for test\n",
        "\n",
        "    # Split the data\n",
        "    train_data[class_name] = all_items[:train_split]\n",
        "    val_data[class_name] = all_items[train_split:val_split]\n",
        "    test_data[class_name] = all_items[val_split:]\n",
        "\n",
        "    # Store split info in metadata\n",
        "    metadata['classes'][class_name] = {\n",
        "        'train': len(train_data[class_name]),\n",
        "        'val': len(val_data[class_name]),\n",
        "        'test': len(test_data[class_name]),\n",
        "        'total': len(all_items)\n",
        "    }\n",
        "\n",
        "    metadata['splits']['train'] += len(train_data[class_name])\n",
        "    metadata['splits']['val'] += len(val_data[class_name])\n",
        "    metadata['splits']['test'] += len(test_data[class_name])\n",
        "    metadata['total_images'] += len(all_items)\n",
        "\n",
        "    print(f\"{class_name:20s}: Train={len(train_data[class_name]):3d} | Val={len(val_data[class_name]):2d} | Test={len(test_data[class_name]):2d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48556611",
      "metadata": {
        "id": "48556611",
        "outputId": "af7f2836-805b-4063-bda2-642c6a7a060e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ’¾ STEP 6: SAVING IMAGES TO DISK\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ PART A: Saving Classification Images...\n",
            "   Format: Cropped objects, 224x224 pixels\n",
            "\n",
            "ğŸ“‚ Processing TRAIN split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Processing VAL split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 31.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Processing TEST split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 31.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… CLASSIFICATION IMAGES SAVED!\n",
            "======================================================================\n",
            "ğŸ“Š Train: 1750 images\n",
            "ğŸ“Š Val:   375 images\n",
            "ğŸ“Š Test:  375 images\n",
            "ğŸ“Š Total: 2500 images\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ’¾ STEP 6: SAVING IMAGES TO DISK\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "# PART A: SAVE CLASSIFICATION IMAGES\n",
        "\n",
        "\n",
        "print(\"ğŸ“ PART A: Saving Classification Images...\")\n",
        "print(\"   Format: Cropped objects, 224x224 pixels\\n\")\n",
        "\n",
        "classification_stats = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "# Process each split\n",
        "for split_name, split_data in [('train', train_data), ('val', val_data), ('test', test_data)]:\n",
        "\n",
        "    print(f\"ğŸ“‚ Processing {split_name.upper()} split...\")\n",
        "\n",
        "    # Process each class\n",
        "    for class_name, items in tqdm(split_data.items(), desc=f\"  {split_name}\"):\n",
        "\n",
        "        class_folder = f\"{BASE_DIR}/classification/{split_name}/{class_name}\"\n",
        "\n",
        "        # Save each image\n",
        "        for img_idx, item in enumerate(items):\n",
        "\n",
        "            img = item['image']\n",
        "            annotations = item['annotations']\n",
        "            bboxes = annotations['bbox']\n",
        "            categories = annotations['category']\n",
        "\n",
        "            class_id = SELECTED_CLASSES[class_name]\n",
        "\n",
        "            # Find bbox for this class\n",
        "            for bbox, cat_id in zip(bboxes, categories):\n",
        "                if cat_id == class_id:\n",
        "                    x, y, w, h = bbox\n",
        "\n",
        "                    try:\n",
        "                        # Crop and resize\n",
        "                        cropped_img = img.crop((x, y, x + w, y + h))\n",
        "                        cropped_img = cropped_img.resize((224, 224), Image.LANCZOS)\n",
        "\n",
        "                        # Save\n",
        "                        img_filename = f\"{class_name}_{split_name}_{img_idx:04d}.jpg\"\n",
        "                        img_path = os.path.join(class_folder, img_filename)\n",
        "                        cropped_img.save(img_path, quality=95)\n",
        "\n",
        "                        classification_stats[split_name] += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Error: {class_name} image {img_idx}: {e}\")\n",
        "\n",
        "                    break\n",
        "\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… CLASSIFICATION IMAGES SAVED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ğŸ“Š Train: {classification_stats['train']} images\")\n",
        "print(f\"ğŸ“Š Val:   {classification_stats['val']} images\")\n",
        "print(f\"ğŸ“Š Test:  {classification_stats['test']} images\")\n",
        "print(f\"ğŸ“Š Total: {sum(classification_stats.values())} images\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df35bbc9",
      "metadata": {
        "id": "df35bbc9",
        "outputId": "41f8cda0-6ded-4592-92e5-486bb6bf77a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ“ PART B: Saving Detection Images & Annotations...\n",
            "   Format: Full images with YOLO .txt labels\n",
            "\n",
            "ğŸ“Š Total detection images: 2125\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving detection data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2125/2125 [00:05<00:00, 366.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… DETECTION DATASET CREATED!\n",
            "======================================================================\n",
            "ğŸ“Š Images:     2125\n",
            "ğŸ“Š Labels:     2125\n",
            "ğŸ“Š Objects:    10986\n",
            "ğŸ“Š Avg/image:  5.17\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# PART B: SAVE DETECTION IMAGES (YOLO FORMAT)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“ PART B: Saving Detection Images & Annotations...\")\n",
        "print(\"   Format: Full images with YOLO .txt labels\\n\")\n",
        "\n",
        "detection_stats = {'images': 0, 'annotations': 0, 'objects': 0}\n",
        "\n",
        "# COCO to YOLO class mapping\n",
        "coco_to_yolo = {class_id: idx for idx, class_id in enumerate(SELECTED_CLASSES.values())}\n",
        "\n",
        "# Combine train + val for detection\n",
        "all_detection_data = []\n",
        "for class_name in SELECTED_CLASSES.keys():\n",
        "    all_detection_data.extend(train_data.get(class_name, []))\n",
        "    all_detection_data.extend(val_data.get(class_name, []))\n",
        "\n",
        "print(f\"ğŸ“Š Total detection images: {len(all_detection_data)}\\n\")\n",
        "\n",
        "# Save images and create YOLO labels\n",
        "for img_idx, item in enumerate(tqdm(all_detection_data, desc=\"Saving detection data\")):\n",
        "\n",
        "    img = item['image']\n",
        "    img_width, img_height = img.size\n",
        "\n",
        "    # Save full image\n",
        "    img_filename = f\"image_{img_idx:06d}.jpg\"\n",
        "    img_path = os.path.join(f\"{BASE_DIR}/detection/images\", img_filename)\n",
        "    img.save(img_path, quality=95)\n",
        "    detection_stats['images'] += 1\n",
        "\n",
        "    # Get annotations\n",
        "    annotations = item['annotations']\n",
        "    bboxes = annotations['bbox']\n",
        "    categories = annotations['category']\n",
        "\n",
        "    # Create YOLO annotation\n",
        "    label_filename = f\"image_{img_idx:06d}.txt\"\n",
        "    label_path = os.path.join(f\"{BASE_DIR}/detection/labels\", label_filename)\n",
        "\n",
        "    yolo_annotations = []\n",
        "    objects_count = 0\n",
        "\n",
        "    for bbox, cat_id in zip(bboxes, categories):\n",
        "        if cat_id in coco_to_yolo:\n",
        "            x, y, w, h = bbox\n",
        "\n",
        "            # Convert to YOLO format (normalized)\n",
        "            x_center = (x + w/2) / img_width\n",
        "            y_center = (y + h/2) / img_height\n",
        "            w_norm = w / img_width\n",
        "            h_norm = h / img_height\n",
        "\n",
        "            yolo_class_id = coco_to_yolo[cat_id]\n",
        "            yolo_line = f\"{yolo_class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\"\n",
        "            yolo_annotations.append(yolo_line)\n",
        "            objects_count += 1\n",
        "\n",
        "    # Save label file\n",
        "    if yolo_annotations:\n",
        "        with open(label_path, 'w') as f:\n",
        "            f.write('\\n'.join(yolo_annotations))\n",
        "        detection_stats['annotations'] += 1\n",
        "        detection_stats['objects'] += objects_count\n",
        "\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… DETECTION DATASET CREATED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ğŸ“Š Images:     {detection_stats['images']}\")\n",
        "print(f\"ğŸ“Š Labels:     {detection_stats['annotations']}\")\n",
        "print(f\"ğŸ“Š Objects:    {detection_stats['objects']}\")\n",
        "print(f\"ğŸ“Š Avg/image:  {detection_stats['objects']/detection_stats['images']:.2f}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9222e7a",
      "metadata": {
        "id": "f9222e7a",
        "outputId": "1b1c0426-9c1d-4aad-d29b-fca10926226d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Creating YOLO configuration file...\n",
            "\n",
            "âœ… Created: smartvision_dataset/detection/data.yaml\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PART C: CREATE YOLO CONFIG FILE\n",
        "\n",
        "print(\"ğŸ“ Creating YOLO configuration file...\\n\")\n",
        "\n",
        "yaml_content = f\"\"\"# SmartVision Dataset - YOLOv8 Configuration\n",
        "path: {os.path.abspath(BASE_DIR)}/detection\n",
        "train: images\n",
        "val: images\n",
        "\n",
        "names:\n",
        "  0: person\n",
        "  1: bicycle\n",
        "  2: car\n",
        "  3: motorcycle\n",
        "  4: airplane\n",
        "  5: bus\n",
        "  6: truck\n",
        "  7: traffic light\n",
        "  8: stop sign\n",
        "  9: bench\n",
        "  10: bird\n",
        "  11: cat\n",
        "  12: dog\n",
        "  13: horse\n",
        "  14: cow\n",
        "  15: elephant\n",
        "  16: bottle\n",
        "  17: cup\n",
        "  18: bowl\n",
        "  19: pizza\n",
        "  20: cake\n",
        "  21: chair\n",
        "  22: couch\n",
        "  23: bed\n",
        "  24: potted plant\n",
        "\n",
        "nc: 25\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = f\"{BASE_DIR}/detection/data.yaml\"\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"âœ… Created: {yaml_path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2390e9",
      "metadata": {
        "id": "6a2390e9",
        "outputId": "d10d6f0a-ff54-4c95-c1fc-ac747b79c580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Saving metadata...\n",
            "\n",
            "âœ… Saved: smartvision_dataset/dataset_metadata.json\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PART D: SAVE METADATA\n",
        "\n",
        "print(\"ğŸ“Š Saving metadata...\\n\")\n",
        "\n",
        "metadata['classification'] = classification_stats\n",
        "metadata['detection'] = detection_stats\n",
        "metadata['dataset_path'] = os.path.abspath(BASE_DIR)\n",
        "\n",
        "metadata_path = f\"{BASE_DIR}/dataset_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, indent=2, fp=f)\n",
        "\n",
        "print(f\"âœ… Saved: {metadata_path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f216bcbb",
      "metadata": {
        "id": "f216bcbb",
        "outputId": "fcaea36c-1e6a-40c9-cb63-132ab8f317d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ‰ DATASET SETUP COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ Location: /Users/subhisapple/Desktop/SmartVision_AI/smartvision_dataset\n",
            "\n",
            "ğŸ“‚ Classification Dataset:\n",
            "   â”œâ”€ Train:  1750 images (70%)\n",
            "   â”œâ”€ Val:    375 images (15%)\n",
            "   â”œâ”€ Test:   375 images (15%)\n",
            "   â””â”€ Total:  2500 cropped images (224x224)\n",
            "\n",
            "ğŸ“‚ Detection Dataset:\n",
            "   â”œâ”€ Images: 2125 full images\n",
            "   â”œâ”€ Labels: 2125 YOLO .txt files\n",
            "   â””â”€ Objects: 10986 annotated objects\n",
            "\n",
            "======================================================================\n",
            "âœ… LEARNERS CAN NOW START:\n",
            "======================================================================\n",
            "Step 7:  Exploratory Data Analysis (EDA)\n",
            "Step 8:  Train Classification Models\n",
            "Step 9:  Train YOLO Detection Model\n",
            "Step 10: Build Streamlit Application\n",
            "Step 11: Deploy to Hugging Face Spaces\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ğŸ‰ DATASET SETUP COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "print(f\"ğŸ“ Location: {os.path.abspath(BASE_DIR)}\")\n",
        "print()\n",
        "print(\"ğŸ“‚ Classification Dataset:\")\n",
        "print(f\"   â”œâ”€ Train:  {classification_stats['train']} images (70%)\")\n",
        "print(f\"   â”œâ”€ Val:    {classification_stats['val']} images (15%)\")\n",
        "print(f\"   â”œâ”€ Test:   {classification_stats['test']} images (15%)\")\n",
        "print(f\"   â””â”€ Total:  {sum(classification_stats.values())} cropped images (224x224)\")\n",
        "print()\n",
        "print(\"ğŸ“‚ Detection Dataset:\")\n",
        "print(f\"   â”œâ”€ Images: {detection_stats['images']} full images\")\n",
        "print(f\"   â”œâ”€ Labels: {detection_stats['annotations']} YOLO .txt files\")\n",
        "print(f\"   â””â”€ Objects: {detection_stats['objects']} annotated objects\")\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… LEARNERS CAN NOW START:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Step 7:  Exploratory Data Analysis (EDA)\")\n",
        "print(\"Step 8:  Train Classification Models\")\n",
        "print(\"Step 9:  Train YOLO Detection Model\")\n",
        "print(\"Step 10: Build Streamlit Application\")\n",
        "print(\"Step 11: Deploy to Hugging Face Spaces\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980b4836",
      "metadata": {
        "id": "980b4836"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}